#!/usr/bin/env python3
"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¢Ù¾Ø¯ÛŒØª Ø®ÙˆØ¯Ú©Ø§Ø± Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ù‡Ø§Ù†ÛŒ
Ù‡Ø± Û³Û° Ø¯Ù‚ÛŒÙ‚Ù‡ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯
"""

import requests
import re
from datetime import datetime

# Ù„ÛŒØ³Øª Ù…Ù†Ø§Ø¨Ø¹ Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø¬Ù‡Ø§Ù†ÛŒ
PROXY_SOURCES = [
    "https://raw.githubusercontent.com/mahdibland/ShadowsocksAggregator/master/sub/sub_merge.txt",
    "https://raw.githubusercontent.com/freefq/free/master/v2",
    "https://raw.githubusercontent.com/ermaozi/get_subscribe/main/subscribe/v2ray.txt",
    "https://raw.githubusercontent.com/aiboboxx/v2rayfree/main/v2",
    "https://raw.githubusercontent.com/Pawdroid/Free-servers/main/sub",
]

def extract_proxy_links(text):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø§Ø² Ù…ØªÙ†"""
    patterns = [
        r'vless://[^\s]+',
        r'vmess://[^\s]+',
        r'trojan://[^\s]+',
        r'ss://[^\s]+',
    ]
    
    links = []
    for pattern in patterns:
        found = re.findall(pattern, text)
        links.extend(found)
    return links

def fetch_proxies_from_source(url):
    """Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒÙ†Ú© Ø§Ø² ÛŒÚ© Ù…Ù†Ø¨Ø¹"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=15)
        if response.status_code == 200:
            proxies = extract_proxy_links(response.text)
            print(f"âœ… {url.split('/')[-1]}: {len(proxies)} Ù„ÛŒÙ†Ú©")
            return proxies
        return []
    except:
        return []

def update_proxies_file():
    """Ø¢Ù¾Ø¯ÛŒØª ÙØ§ÛŒÙ„ texts.txt"""
    print("=" * 50)
    print(f"ğŸ”„ Ø´Ø±ÙˆØ¹ Ø¢Ù¾Ø¯ÛŒØª - {datetime.now().strftime('%H:%M:%S')}")
    
    # Ø®ÙˆØ§Ù†Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
    try:
        with open('texts.txt', 'r', encoding='utf-8') as f:
            existing = [line.strip() for line in f if line.strip()]
    except:
        existing = []
    
    print(f"ğŸ“Š Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯: {len(existing)}")
    
    # Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
    all_new = []
    for source in PROXY_SOURCES:
        new = fetch_proxies_from_source(source)
        all_new.extend(new)
    
    # Ø§Ø¯ØºØ§Ù… Ùˆ Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒ
    unique = list(dict.fromkeys(existing + all_new))
    
    # Ø°Ø®ÛŒØ±Ù‡
    with open('texts.txt', 'w', encoding='utf-8') as f:
        for link in unique:
            f.write(link + '\n')
    
    print(f"ğŸ“ˆ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯: {len(all_new)}")
    print(f"ğŸ“ Ú©Ù„ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§: {len(unique)}")
    print("=" * 50)

if __name__ == "__main__":
    update_proxies_file()
