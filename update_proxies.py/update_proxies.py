#!/usr/bin/env python3
"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¢Ù¾Ø¯ÛŒØª Ø®ÙˆØ¯Ú©Ø§Ø± Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ù‡Ø§Ù†ÛŒ Ù‡Ø± Û³Û° Ø¯Ù‚ÛŒÙ‚Ù‡
"""

import requests
import re
from datetime import datetime

# Ù…Ù†Ø§Ø¨Ø¹ Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø¬Ù‡Ø§Ù†ÛŒ
PROXY_SOURCES = [
    # Ù…Ù†Ø§Ø¨Ø¹ Ø§ØµÙ„ÛŒ
    "https://raw.githubusercontent.com/mahdibland/ShadowsocksAggregator/master/sub/sub_merge.txt",
    "https://raw.githubusercontent.com/freefq/free/master/v2",
    "https://raw.githubusercontent.com/ermaozi/get_subscribe/main/subscribe/v2ray.txt",
    
    # Ù…Ù†Ø§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ
    "https://raw.githubusercontent.com/aiboboxx/v2rayfree/main/v2",
    "https://raw.githubusercontent.com/Pawdroid/Free-servers/main/sub",
]

def extract_proxy_links(text):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø§Ø² Ù…ØªÙ†"""
    patterns = [
        r'vless://[^\s]+',
        r'vmess://[^\s]+', 
        r'trojan://[^\s]+',
        r'ss://[^\s]+',
    ]
    
    links = []
    for pattern in patterns:
        found = re.findall(pattern, text)
        links.extend(found)
    
    return links

def fetch_proxies_from_source(url):
    """Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø§Ø² ÛŒÚ© Ù…Ù†Ø¨Ø¹"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            proxies = extract_proxy_links(response.text)
            print(f"âœ… {url.split('/')[-1]}: {len(proxies)} Ù¾Ø±ÙˆÚ©Ø³ÛŒ")
            return proxies
        else:
            print(f"âš ï¸ {url.split('/')[-1]}: Ø®Ø·Ø§ {response.status_code}")
            return []
            
    except Exception as e:
        print(f"âŒ {url.split('/')[-1]}: {str(e)[:50]}")
        return []

def update_proxies_file():
    """Ø¢Ù¾Ø¯ÛŒØª ÙØ§ÛŒÙ„ texts.txt"""
    print("=" * 50)
    print(f"ğŸ”„ Ø¢Ù¾Ø¯ÛŒØª Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ - {datetime.now().strftime('%H:%M:%S')}")
    
    # Ø®ÙˆØ§Ù†Ø¯Ù† Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
    try:
        with open('texts.txt', 'r', encoding='utf-8') as f:
            existing = [line.strip() for line in f if line.strip()]
    except:
        existing = []
    
    print(f"ğŸ“Š Ù…ÙˆØ¬ÙˆØ¯: {len(existing)} Ù¾Ø±ÙˆÚ©Ø³ÛŒ")
    
    # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø² Ù‡Ù…Ù‡ Ù…Ù†Ø§Ø¨Ø¹
    all_new = []
    for source in PROXY_SOURCES:
        new_proxies = fetch_proxies_from_source(source)
        all_new.extend(new_proxies)
    
    # Ø§Ø¯ØºØ§Ù… Ùˆ Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒ
    unique_proxies = list(dict.fromkeys(existing + all_new))
    
    # Ø°Ø®ÛŒØ±Ù‡
    with open('texts.txt', 'w', encoding='utf-8') as f:
        for proxy in unique_proxies:
            f.write(proxy + '\n')
    
    print("=" * 50)
    print(f"âœ… Ø¢Ù¾Ø¯ÛŒØª Ú©Ø§Ù…Ù„ Ø´Ø¯!")
    print(f"ğŸ“ˆ Ø¬Ø¯ÛŒØ¯: {len(all_new)} | Ú©Ù„: {len(unique_proxies)}")
    
    return len(unique_proxies)

if __name__ == "__main__":
    update_proxies_file()
